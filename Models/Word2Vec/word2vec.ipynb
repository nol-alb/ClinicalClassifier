{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = preprocess(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('European', 'JJ'),\n",
       " ('authorities', 'NNS'),\n",
       " ('fined', 'VBD'),\n",
       " ('Google', 'NNP'),\n",
       " ('a', 'DT'),\n",
       " ('record', 'NN'),\n",
       " ('$', '$'),\n",
       " ('5.1', 'CD'),\n",
       " ('billion', 'CD'),\n",
       " ('on', 'IN'),\n",
       " ('Wednesday', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('abusing', 'VBG'),\n",
       " ('its', 'PRP$'),\n",
       " ('power', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mobile', 'JJ'),\n",
       " ('phone', 'NN'),\n",
       " ('market', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('ordered', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('company', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('alter', 'VB'),\n",
       " ('its', 'PRP$'),\n",
       " ('practices', 'NNS')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  European/JJ\n",
      "  authorities/NNS\n",
      "  fined/VBD\n",
      "  Google/NNP\n",
      "  (NP a/DT record/NN)\n",
      "  $/$\n",
      "  5.1/CD\n",
      "  billion/CD\n",
      "  on/IN\n",
      "  Wednesday/NNP\n",
      "  for/IN\n",
      "  abusing/VBG\n",
      "  its/PRP$\n",
      "  (NP power/NN)\n",
      "  in/IN\n",
      "  (NP the/DT mobile/JJ phone/NN)\n",
      "  (NP market/NN)\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  (NP the/DT company/NN)\n",
      "  to/TO\n",
      "  alter/VB\n",
      "  its/PRP$\n",
      "  practices/NNS)\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'JJ', 'O'),\n",
      " ('authorities', 'NNS', 'O'),\n",
      " ('fined', 'VBD', 'O'),\n",
      " ('Google', 'NNP', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('record', 'NN', 'I-NP'),\n",
      " ('$', '$', 'O'),\n",
      " ('5.1', 'CD', 'O'),\n",
      " ('billion', 'CD', 'O'),\n",
      " ('on', 'IN', 'O'),\n",
      " ('Wednesday', 'NNP', 'O'),\n",
      " ('for', 'IN', 'O'),\n",
      " ('abusing', 'VBG', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('power', 'NN', 'B-NP'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('mobile', 'JJ', 'I-NP'),\n",
      " ('phone', 'NN', 'I-NP'),\n",
      " ('market', 'NN', 'B-NP'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('ordered', 'VBD', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('company', 'NN', 'I-NP'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('alter', 'VB', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('practices', 'NNS', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "pprint(iob_tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE European/JJ)\n",
      "  authorities/NNS\n",
      "  fined/VBD\n",
      "  (PERSON Google/NNP)\n",
      "  a/DT\n",
      "  record/NN\n",
      "  $/$\n",
      "  5.1/CD\n",
      "  billion/CD\n",
      "  on/IN\n",
      "  Wednesday/NNP\n",
      "  for/IN\n",
      "  abusing/VBG\n",
      "  its/PRP$\n",
      "  power/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  mobile/JJ\n",
      "  phone/NN\n",
      "  market/NN\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  the/DT\n",
      "  company/NN\n",
      "  to/TO\n",
      "  alter/VB\n",
      "  its/PRP$\n",
      "  practices/NNS)\n"
     ]
    }
   ],
   "source": [
    "ne_tree = nltk.ne_chunk(pos_tag(word_tokenize(ex)))\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-76a01d9c502b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.0.3-cp38-cp38-win_amd64.whl (11.8 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp38-cp38-win_amd64.whl (112 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.0\n",
      "  Downloading thinc-8.0.1-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Using cached wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.0\n",
      "  Downloading srsly-2.4.0-cp38-cp38-win_amd64.whl (451 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "  Downloading pydantic-1.7.3-cp38-cp38-win_amd64.whl (1.8 MB)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (20.4)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp38-cp38-win_amd64.whl (21 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.47.0)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp38-cp38-win_amd64.whl (6.5 MB)\n",
      "Collecting catalogue<2.1.0,>=2.0.1\n",
      "  Downloading catalogue-2.0.1-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (49.2.0.post20200714)\n",
      "Collecting pathy\n",
      "  Downloading pathy-0.4.0-py3-none-any.whl (36 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
      "  Downloading spacy_legacy-3.0.1-py2.py3-none-any.whl (7.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107102 sha256=c78b2c71bdededbc4919e025eb3b44227e0dab91b52ed3918183fabd7247daec\n",
      "  Stored in directory: c:\\users\\niraj\\appdata\\local\\pip\\cache\\wheels\\11\\73\\9a\\f91ac1f1816436b16423617c5be5db048697ff152a9c4346f2\n",
      "Successfully built smart-open\n",
      "Installing collected packages: murmurhash, cymem, preshed, catalogue, pydantic, srsly, blis, wasabi, thinc, typer, smart-open, pathy, spacy-legacy, spacy\n",
      "Successfully installed blis-0.7.4 catalogue-2.0.1 cymem-2.0.5 murmurhash-1.0.5 pathy-0.4.0 preshed-3.0.5 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.3 spacy-legacy-3.0.1 srsly-2.4.0 thinc-8.0.1 typer-0.3.2 wasabi-0.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en_core_web_sm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-86b5dfc9b16e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'en_core_web_sm'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement en_core_web_sm (from versions: none)\n",
      "ERROR: No matching distribution found for en_core_web_sm\n"
     ]
    }
   ],
   "source": [
    "!pip install en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz (120.8 MB)\n",
      "Requirement already satisfied: spacy>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from en-core-web-md==2.0.0) (3.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (0.7.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (49.2.0.post20200714)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (1.7.3)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (0.3.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (8.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (20.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (0.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (2.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (3.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (2.24.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (2.11.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (1.18.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (3.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (4.47.0)\n",
      "Requirement already satisfied: pathy in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.0.0->en-core-web-md==2.0.0) (0.4.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy>=2.0.0->en-core-web-md==2.0.0) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=2.0.0->en-core-web-md==2.0.0) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=2.0.0->en-core-web-md==2.0.0) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0->en-core-web-md==2.0.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0->en-core-web-md==2.0.0) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0->en-core-web-md==2.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0->en-core-web-md==2.0.0) (2020.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy>=2.0.0->en-core-web-md==2.0.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pathy->spacy>=2.0.0->en-core-web-md==2.0.0) (3.0.0)\n",
      "Building wheels for collected packages: en-core-web-md\n",
      "  Building wheel for en-core-web-md (setup.py): started\n",
      "  Building wheel for en-core-web-md (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-md: filename=en_core_web_md-2.0.0-py3-none-any.whl size=122523225 sha256=f9b141b51a2f279b6afd16b8e63be2197784f28ffcc59cf508e9d8de0b6e13f4\n",
      "  Stored in directory: c:\\users\\niraj\\appdata\\local\\pip\\cache\\wheels\\f2\\1d\\62\\9eb147a5e9a1de1a8275822d6253f6f90fe33a48f3718e772b\n",
      "Successfully built en-core-web-md\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en_core_web_sm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-bd9b35c10143>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'en_core_web_sm'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0 MB)\n",
      "Requirement already satisfied: spacy>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from en-core-web-sm==2.2.0) (3.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.24.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (8.0.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.18.5)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.7.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (49.2.0.post20200714)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.4.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (20.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.5)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.11.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (4.47.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.7.4)\n",
      "Requirement already satisfied: pathy in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.25.9)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pathy->spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py): started\n",
      "  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.0-py3-none-any.whl size=12019125 sha256=84b928d2e364324a718fb2ccf693d35740da09311e86c910b491b4a5d7cc1424\n",
      "  Stored in directory: c:\\users\\niraj\\appdata\\local\\pip\\cache\\wheels\\fc\\31\\e9\\092e6f05b2817c9cb45804a3d1bf2b9bf6575742c01819337c\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py:715: UserWarning: [W094] Model 'en_core_web_sm' (2.2.0) specifies an under-constrained spaCy version requirement: >=2.2.0. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config.cfg from C:\\ProgramData\\Anaconda3\\lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.0\\config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-86b5dfc9b16e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\en_core_web_sm\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m     return load_model_from_path(\n\u001b[0m\u001b[0;32m    515\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model_meta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[0mconfig_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"config.cfg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_config\u001b[1;34m(path, overrides, interpolate)\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE053\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"config.cfg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m         return config.from_disk(\n\u001b[0;32m    547\u001b[0m             \u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E053] Could not read config.cfg from C:\\ProgramData\\Anaconda3\\lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.0\\config.cfg"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilirubin test kit (physical object)\n",
      "Methylphenyltetrahydropyridine (substance)\n",
      "471023\n",
      "864\n"
     ]
    }
   ],
   "source": [
    "baseUrl = 'https://browser.ihtsdotools.org/snowstorm/snomed-ct'\n",
    "edition = 'MAIN'\n",
    "version = '2019-07-31'\n",
    "\n",
    "#Prints fsn of a concept\n",
    "def getConceptById(id):\n",
    "    url = baseUrl + '/browser/' + edition + '/' + version + '/concepts/' + id\n",
    "    response = urlopen(url).read()\n",
    "    data = json.loads(response.decode('utf-8'))\n",
    "\n",
    "    print (data['fsn']['term'])\n",
    "\n",
    "#Prints description by id\n",
    "def getDescriptionById(id):\n",
    "    url = baseUrl + '/' + edition + '/' + version + '/descriptions/' + id\n",
    "    response = urlopen(url).read()\n",
    "    data = json.loads(response.decode('utf-8'))\n",
    "\n",
    "    print (data['term'])\n",
    "\n",
    "#Prints number of concepts with descriptions containing the search term\n",
    "def getConceptsByString(searchTerm):\n",
    "    url = baseUrl + '/browser/' + edition + '/' + version + '/concepts?term=' + quote(searchTerm) + '&activeFilter=true&offset=0&limit=50'\n",
    "    response = urlopen(url).read()\n",
    "    data = json.loads(response.decode('utf-8'))\n",
    "\n",
    "    print (data['total'])\n",
    "\n",
    "#Prints number of descriptions containing the search term with a specific semantic tag\n",
    "def getDescriptionsByStringFromProcedure(searchTerm, semanticTag):\n",
    "    url = baseUrl + '/browser/' + edition + '/' + version + '/descriptions?term=' + quote(searchTerm) + '&conceptActive=true&semanticTag=' + quote(semanticTag) + '&groupByConcept=false&searchMode=STANDARD&offset=0&limit=50'\n",
    "    response = urlopen(url).read()\n",
    "    data = json.loads(response.decode('utf-8'))\n",
    "\n",
    "    print (data['totalElements'])\n",
    "\n",
    "getConceptById('109152007')\n",
    "getDescriptionById('679406011')\n",
    "getConceptsByString('heart attack')\n",
    "getDescriptionsByStringFromProcedure('heart', 'procedure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conceptId': '109152007', 'fsn': {'term': 'Bilirubin test kit (physical object)', 'lang': 'en'}, 'pt': {'term': 'Bilirubin test kit', 'lang': 'en'}, 'active': True, 'effectiveTime': '20020131', 'released': True, 'releasedEffectiveTime': 20020131, 'moduleId': '900000000000207008', 'definitionStatus': 'PRIMITIVE', 'descriptions': [{'active': True, 'moduleId': '900000000000207008', 'released': True, 'releasedEffectiveTime': 20170731, 'descriptionId': '173687013', 'term': 'Bilirubin test kit', 'conceptId': '109152007', 'typeId': '900000000000013009', 'acceptabilityMap': {'900000000000509007': 'PREFERRED', '900000000000508004': 'PREFERRED'}, 'type': 'SYNONYM', 'lang': 'en', 'caseSignificance': 'CASE_INSENSITIVE', 'effectiveTime': '20170731'}, {'active': True, 'moduleId': '900000000000207008', 'released': True, 'releasedEffectiveTime': 20170731, 'descriptionId': '3324354018', 'term': 'Bilirubin test kit (physical object)', 'conceptId': '109152007', 'typeId': '900000000000003001', 'acceptabilityMap': {'900000000000509007': 'PREFERRED', '900000000000508004': 'PREFERRED'}, 'type': 'FSN', 'lang': 'en', 'caseSignificance': 'CASE_INSENSITIVE', 'effectiveTime': '20170731'}, {'active': False, 'moduleId': '900000000000207008', 'released': True, 'releasedEffectiveTime': 20170131, 'descriptionId': '608781019', 'term': 'Bilirubin test kit (substance)', 'conceptId': '109152007', 'typeId': '900000000000003001', 'acceptabilityMap': {}, 'type': 'FSN', 'inactivationIndicator': 'OUTDATED', 'lang': 'en', 'caseSignificance': 'INITIAL_CHARACTER_CASE_INSENSITIVE', 'effectiveTime': '20170131'}], 'classAxioms': [{'axiomId': 'ccce091e-41d3-4d8c-aa35-2fdf9354a604', 'moduleId': '900000000000207008', 'active': True, 'released': True, 'definitionStatusId': '900000000000074008', 'relationships': [{'active': True, 'moduleId': '900000000000207008', 'released': False, 'sourceId': '109152007', 'destinationId': '385387009', 'typeId': '116680003', 'type': {'conceptId': '116680003', 'definitionStatus': 'PRIMITIVE', 'moduleId': '900000000000012004', 'fsn': {'term': 'Is a (attribute)', 'lang': 'en'}, 'pt': {'term': 'Is a', 'lang': 'en'}, 'id': '116680003'}, 'target': {'conceptId': '385387009', 'definitionStatus': 'PRIMITIVE', 'moduleId': '900000000000207008', 'fsn': {'term': 'Test kit (physical object)', 'lang': 'en'}, 'pt': {'term': 'Test kit', 'lang': 'en'}, 'id': '385387009'}, 'groupId': 0, 'modifier': 'EXISTENTIAL', 'characteristicType': 'STATED_RELATIONSHIP'}], 'definitionStatus': 'PRIMITIVE', 'id': 'ccce091e-41d3-4d8c-aa35-2fdf9354a604', 'effectiveTime': 20190731}], 'gciAxioms': [], 'relationships': [{'active': True, 'moduleId': '900000000000207008', 'released': True, 'releasedEffectiveTime': 20030131, 'relationshipId': '1913966028', 'sourceId': '109152007', 'destinationId': '385387009', 'typeId': '116680003', 'type': {'conceptId': '116680003', 'definitionStatus': 'PRIMITIVE', 'moduleId': '900000000000012004', 'fsn': {'term': 'Is a (attribute)', 'lang': 'en'}, 'pt': {'term': 'Is a', 'lang': 'en'}, 'id': '116680003'}, 'target': {'conceptId': '385387009', 'definitionStatus': 'PRIMITIVE', 'moduleId': '900000000000207008', 'fsn': {'term': 'Test kit (physical object)', 'lang': 'en'}, 'pt': {'term': 'Test kit', 'lang': 'en'}, 'id': '385387009'}, 'groupId': 0, 'modifier': 'EXISTENTIAL', 'characteristicType': 'INFERRED_RELATIONSHIP', 'effectiveTime': '20030131', 'id': '1913966028'}, {'active': False, 'moduleId': '900000000000207008', 'released': True, 'releasedEffectiveTime': 20020731, 'relationshipId': '30008024', 'sourceId': '109152007', 'destinationId': '2949005', 'typeId': '116680003', 'type': {'conceptId': '116680003', 'definitionStatus': 'PRIMITIVE', 'moduleId': '900000000000012004', 'fsn': {'term': 'Is a (attribute)', 'lang': 'en'}, 'pt': {'term': 'Is a', 'lang': 'en'}, 'id': '116680003'}, 'target': {'conceptId': '2949005', 'definitionStatus': 'PRIMITIVE', 'moduleId': '900000000000207008', 'fsn': {'term': 'Diagnostic aid (product)', 'lang': 'en'}, 'pt': {'term': 'Diagnostic aid', 'lang': 'en'}, 'id': '2949005'}, 'groupId': 0, 'modifier': 'EXISTENTIAL', 'characteristicType': 'INFERRED_RELATIONSHIP', 'effectiveTime': '20020731', 'id': '30008024'}, {'active': False, 'moduleId': '900000000000207008', 'released': True, 'releasedEffectiveTime': 20030131, 'relationshipId': '1653305021', 'sourceId': '109152007', 'destinationId': '373782009', 'typeId': '116680003', 'type': {'conceptId': '116680003', 'definitionStatus': 'PRIMITIVE', 'moduleId': '900000000000012004', 'fsn': {'term': 'Is a (attribute)', 'lang': 'en'}, 'pt': {'term': 'Is a', 'lang': 'en'}, 'id': '116680003'}, 'target': {'conceptId': '373782009', 'definitionStatus': 'PRIMITIVE', 'moduleId': '900000000000207008', 'fsn': {'term': 'Diagnostic substance (substance)', 'lang': 'en'}, 'pt': {'term': 'Diagnostic substance', 'lang': 'en'}, 'id': '373782009'}, 'groupId': 0, 'modifier': 'EXISTENTIAL', 'characteristicType': 'INFERRED_RELATIONSHIP', 'effectiveTime': '20030131', 'id': '1653305021'}, {'active': False, 'moduleId': '900000000000207008', 'released': True, 'releasedEffectiveTime': 20190731, 'relationshipId': '3851669025', 'sourceId': '109152007', 'destinationId': '385387009', 'typeId': '116680003', 'type': {'conceptId': '116680003', 'definitionStatus': 'PRIMITIVE', 'moduleId': '900000000000012004', 'fsn': {'term': 'Is a (attribute)', 'lang': 'en'}, 'pt': {'term': 'Is a', 'lang': 'en'}, 'id': '116680003'}, 'target': {'conceptId': '385387009', 'definitionStatus': 'PRIMITIVE', 'moduleId': '900000000000207008', 'fsn': {'term': 'Test kit (physical object)', 'lang': 'en'}, 'pt': {'term': 'Test kit', 'lang': 'en'}, 'id': '385387009'}, 'groupId': 0, 'modifier': 'EXISTENTIAL', 'characteristicType': 'STATED_RELATIONSHIP', 'effectiveTime': '20190731', 'id': '3851669025'}]}\n"
     ]
    }
   ],
   "source": [
    "baseUrl = 'https://browser.ihtsdotools.org/snowstorm/snomed-ct'\n",
    "edition = 'MAIN'\n",
    "version = '2019-07-31'\n",
    "\n",
    "#Prints fsn of a concept\n",
    "def getConceptById(id):\n",
    "    url = baseUrl + '/browser/' + edition + '/' + version + '/concepts/' + id\n",
    "    response = urlopen(url).read()\n",
    "    data = json.loads(response.decode('utf-8'))\n",
    "    print(data)\n",
    "getConceptById('109152007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-05ba2fd7a670>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-29-05ba2fd7a670>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    https://browser.ihtsdotools.org/snowstorm/snomed-ct/browser/MAIN/2019-07-31/concepts/109152007\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://browser.ihtsdotools.org/snowstorm/snomed-ct/browser/MAIN/2019-07-31/concepts/109152007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-2274cc41601c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'https://www.mtsamples.com/site/pages/browse.asp?type=89-Discharge%20Summary'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m             response = self.parent.error(\n\u001b[0m\u001b[0;32m    641\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "url='https://www.mtsamples.com/site/pages/browse.asp?type=89-Discharge%20Summary'\n",
    "response = urlopen(url).read()\n",
    "data = json.loads(response.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-50-7f19940deee0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-50-7f19940deee0>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ************* NER MODEL*************\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "************************* NER MODEL *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rights at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood', '-', 'cuting', 'fishery', 'rights', '.', 'ready', 'becuase', 'rights', 'valuable', ',', '\\n', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Create list of word tokens after removing stopwords\n",
    "filtered_sentence =[] \n",
    "\n",
    "for word in token_list:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    if lexeme.is_stop == False:\n",
    "        filtered_sentence.append(word) \n",
    "#print(token_list)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "************* NLTK *******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rights', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'lesser', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
      "['He', 'determine', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claim', 'wood-cuting', 'fishery', 'right', '.', 'He', 'ready', 'becuase', 'right', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vague', 'idea', 'wood', 'river', 'question', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rights at once. He was the more ready to do this becuase the rights had become much lesser valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "word_tokens = word_tokenize(text) \n",
    "    \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "print(filtered_sentence) \n",
    "\n",
    "lemma_word = []\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for w in filtered_sentence:\n",
    "    word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n",
    "    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "    lemma_word.append(word3)\n",
    "print(lemma_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-55-d9b1c18804c9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-55-d9b1c18804c9>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    **************  Word2Vec ******************\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "**************  Word2Vec ******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello',\n",
       "  'hello',\n",
       "  'hello',\n",
       "  'hello',\n",
       "  'hello',\n",
       "  'hello',\n",
       "  'hello',\n",
       "  'hello',\n",
       "  'hello',\n",
       "  'hgllo']]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello hello hello hello hello hello hello hello hello hgllo\"\n",
    "\n",
    "# Note the .lower() as upper and lowercase does not matter in our implementation\n",
    "# [['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'and', 'exciting']]\n",
    "corpus = [[word.lower() for word in text.split()]]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {'window_size':2,'n':10,'epochs': 50,'learning_rate':0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "  def __init__(self):\n",
    "    self.n = settings['n']\n",
    "    self.lr = settings['learning_rate']\n",
    "    self.epochs = settings['epochs']\n",
    "    self.window = settings['window_size']\n",
    "\n",
    "  def generate_training_data(self, settings, corpus):\n",
    "    # Find unique word counts using dictonary\n",
    "    word_counts = defaultdict(int)\n",
    "    for row in corpus:\n",
    "      for word in row:\n",
    "        word_counts[word] += 1\n",
    "    ## How many unique words in vocab? 9\n",
    "    self.v_count = len(word_counts.keys())\n",
    "    # Generate Lookup Dictionaries (vocab)\n",
    "    self.words_list = list(word_counts.keys())\n",
    "    # Generate word:index\n",
    "    self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
    "    # Generate index:word\n",
    "    self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
    "\n",
    "    training_data = []\n",
    "    # Cycle through each sentence in corpus\n",
    "    for sentence in corpus:\n",
    "      sent_len = len(sentence)\n",
    "      # Cycle through each word in sentence\n",
    "      for i, word in enumerate(sentence):\n",
    "        # Convert target word to one-hot\n",
    "        w_target = self.word2onehot(sentence[i])\n",
    "        # Cycle through context window\n",
    "        w_context = []\n",
    "        # Note: window_size 2 will have range of 5 values\n",
    "        for j in range(i - self.window, i + self.window+1):\n",
    "          # Criteria for context word \n",
    "          # 1. Target word cannot be context word (j != i)\n",
    "          # 2. Index must be greater or equal than 0 (j >= 0) - if not list index out of range\n",
    "          # 3. Index must be less or equal than length of sentence (j <= sent_len-1) - if not list index out of range \n",
    "          if j != i and j <= sent_len-1 and j >= 0:\n",
    "            # Append the one-hot representation of word to w_context\n",
    "            w_context.append(self.word2onehot(sentence[j]))\n",
    "            # print(sentence[i], sentence[j]) \n",
    "            # training_data contains a one-hot representation of the target word and context words\n",
    "        training_data.append([w_target, w_context])\n",
    "    return np.array(training_data)\n",
    "  \n",
    "  def word2onehot(self, word):\n",
    "    # word_vec - initialise a blank vector\n",
    "    word_vec = [0 for i in range(0, self.v_count)] # Alternative - np.zeros(self.v_count)\n",
    "    # Get ID of word from word_index\n",
    "    word_index = self.word_index[word]\n",
    "    # Change value from 0 to 1 according to ID of the word\n",
    "    word_vec[word_index] = 1\n",
    "    return word_vec\n",
    "  \n",
    "  def train(self, training_data):\n",
    "    self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))\n",
    "    self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))\n",
    "    \n",
    "  def forward_pass(self, x):\n",
    "    # x is one-hot vector for target word, shape - 9x1\n",
    "    # Run through first matrix (w1) to get hidden layer - 10x9 dot 9x1 gives us 10x1\n",
    "    h = np.dot(self.w1.T, x)\n",
    "    # Dot product hidden layer with second matrix (w2) - 9x10 dot 10x1 gives us 9x1\n",
    "    u = np.dot(self.w2.T, h)\n",
    "    # Run 1x9 through softmax to force each element to range of [0, 1] - 1x8\n",
    "    y_c = self.softmax(u)\n",
    "    return y_c, h, u\n",
    "  \n",
    "  def softmax(self, x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "  # 1. For a target word, calculate difference between y_pred and each of the context words\n",
    "  # 2. Sum up the differences using np.sum to give us the error for this particular target word\n",
    "  #EI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0)\n",
    "\n",
    "  # Backpropagation\n",
    "  # We use SGD to backpropagate errors - calculate loss on the output layer \n",
    "  #self.backprop(EI, h, w_t)\n",
    "\n",
    "  # Calculate loss\n",
    "  # There are 2 parts to the loss function\n",
    "  # Part 1: -ve sum of all the output +\n",
    "  # Part 2: length of context words * log of sum for all elements (exponential-ed) in the output layer before softmax (u)\n",
    "  # Note: word.index(1) returns the index in the context word vector with value 1\n",
    "  # Note: u[word.index(1)] returns the value of the output layer before softmax\n",
    "  #self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
    "  #print('Epoch:', i, \"Loss:\", self.loss)\n",
    "  \n",
    "  def backprop(self, e, h, x):\n",
    "    # https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.outer.html\n",
    "    # Column vector EI represents row-wise sum of prediction errors across each context word for the current center word\n",
    "    # Going backwards, we need to take derivative of E with respect of w2\n",
    "    # h - shape 10x1, e - shape 9x1, dl_dw2 - shape 10x9\n",
    "    dl_dw2 = np.outer(h, e)\n",
    "    # x - shape 1x8, w2 - 5x8, e.T - 8x1\n",
    "    # x - 1x8, np.dot() - 5x1, dl_dw1 - 8x5\n",
    "    dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
    "    # Update weights\n",
    "    self.w1 = self.w1 - (self.lr * dl_dw1)\n",
    "    self.w2 = self.w2 - (self.lr * dl_dw2)\n",
    "      \n",
    "  def word_vec(self, word):\n",
    "    w_index = self.word_index[word]\n",
    "    v_w = self.w1[w_index]\n",
    "    return v_w\n",
    "  \n",
    "  def vec_sim(self, word, top_n):\n",
    "    v_w1 = self.word_vec(word)\n",
    "    word_sim = {}\n",
    "\n",
    "    for i in range(self.v_count):\n",
    "      # Find the similary score for each word in vocab\n",
    "      v_w2 = self.w1[i]\n",
    "      theta_sum = np.dot(v_w1, v_w2)\n",
    "      theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
    "      theta = theta_sum / theta_den\n",
    "\n",
    "      word = self.index_word[i]\n",
    "      word_sim[word] = theta\n",
    "\n",
    "    words_sorted = sorted(word_sim.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    for word, sim in words_sorted[:top_n]:\n",
    "      print(word, sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = word2vec()\n",
    "# Numpy ndarray with one-hot representation for [target_word, context_words]\n",
    "training_data = w2v.generate_training_data(settings, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[list([1, 0]), list([[1, 0], [1, 0]])],\n",
       "       [list([1, 0]), list([[1, 0], [1, 0], [1, 0]])],\n",
       "       [list([1, 0]), list([[1, 0], [1, 0], [1, 0], [1, 0]])],\n",
       "       [list([1, 0]), list([[1, 0], [1, 0], [1, 0], [1, 0]])],\n",
       "       [list([1, 0]), list([[1, 0], [1, 0], [1, 0], [1, 0]])],\n",
       "       [list([1, 0]), list([[1, 0], [1, 0], [1, 0], [1, 0]])],\n",
       "       [list([1, 0]), list([[1, 0], [1, 0], [1, 0], [1, 0]])],\n",
       "       [list([1, 0]), list([[1, 0], [1, 0], [1, 0], [0, 1]])],\n",
       "       [list([1, 0]), list([[1, 0], [1, 0], [0, 1]])],\n",
       "       [list([0, 1]), list([[1, 0], [1, 0]])]], dtype=object)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(w2v.epochs):\n",
    "    w2v.train(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = w2v.word_vec(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.42449835, -0.77414975,  0.08171911, -0.28020617, -0.68854559,\n",
       "       -0.85839747, -0.33396541,  0.88135877, -0.59625071,  0.98745925])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello 1.0\n",
      "hgllo -0.11794247225366143\n"
     ]
    }
   ],
   "source": [
    "w2v.vec_sim(\"hello\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-0f93dcc62c54>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-0f93dcc62c54>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    **************** BERT ***********************\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "**************** BERT ************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\programdata\\anaconda3\\lib\\site-packages (0.4.1.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.23.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.18.5)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.47.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.5)\n",
      "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.3.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.95)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (2020.6.8)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\niraj\\appdata\\roaming\\python\\python38\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.1)\n",
      "Requirement already satisfied: sacremoses in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 405M/405M [04:29<00:00, 1.50MB/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"A 74 year old gentleman, known case of Type 2 diabetes mellitus, hypertension, acute inferior wall MI with complete heart block, presented with complaints of chest pain since 10 days.\", \n",
    "       \"Physical examination showed that patient is conscious and oriented. Pulse-94/min; BP-120/80 mmHg; CVS-S1S2 normal; RS-NVBS; ABD-Soft .\", \n",
    "       \"Brad came to dinner with us.\",\n",
    "       \"On admission ECG showed sinus rhythm, QS with T inversion in III, AVF . Echo showed RWMA involving inferior wall with adequate LV systolic function .\",\n",
    "       \"TIMI III flow achieved with good end result .\",\n",
    "       \"i dont have have cancer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_embeddings = sbert_model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"i dont have heart ache in evening\"\n",
    "query_vec = sbert_model.encode([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence =  A 74 year old gentleman, known case of Type 2 diabetes mellitus, hypertension, acute inferior wall MI with complete heart block, presented with complaints of chest pain since 10 days. ; similarity =  -0.009621797\n",
      "Sentence =  Physical examination showed that patient is conscious and oriented. Pulse-94/min; BP-120/80 mmHg; CVS-S1S2 normal; RS-NVBS; ABD-Soft . ; similarity =  0.34176022\n",
      "Sentence =  Brad came to dinner with us. ; similarity =  0.18913847\n",
      "Sentence =  On admission ECG showed sinus rhythm, QS with T inversion in III, AVF . Echo showed RWMA involving inferior wall with adequate LV systolic function . ; similarity =  0.29188272\n",
      "Sentence =  TIMI III flow achieved with good end result . ; similarity =  0.4672234\n",
      "Sentence =  i dont have have cancer ; similarity =  0.7527088\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "  sim = cosine(query_vec, sbert_model.encode([sent])[0])\n",
    "  print(\"Sentence = \", sent, \"; similarity = \", sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
